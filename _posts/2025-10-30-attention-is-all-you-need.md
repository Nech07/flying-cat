---
title: "Attention Is All You Need"
paper_title: "Attention Is All You Need"
paper_authors: "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin"
paper_venue: "NeurIPS"
paper_year: 2017
paper_link: https://arxiv.org/abs/1706.03762
paper_code: https://github.com/jadore801120/attention-is-all-you-need-pytorch
paper_tags:
  - transformers
  - sequence-to-sequence
  - attention
key_takeaways: |
  - The Transformer discards recurrence in favor of stacked self-attention blocks that scale well on modern hardware.
  - Multi-head attention lets the model jointly attend to information from different representation subspaces.
  - Positional encodings inject order without recurrence, enabling parallel training and efficient long-range modeling.
  - Label smoothing and residual connections play an outsize role in stabilizing deep attention stacks.
  - Transformer achieves state-of-the-art machine translation with significantly less training cost than RNN counterparts.
read_time: 6
---

## Main concept

## Main advantages

## Experiments results

## Practical model application

## technical details

## Limitations

## Future Directions

## Overview
